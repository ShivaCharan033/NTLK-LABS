{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41cde5f-2fcd-4077-9e8e-0dc3eeb5dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports useful packages, e.g. pandas, numpy, random.\n",
    "import pandas as pd\n",
    "# imports Feature Engineered/Supervised models from scikit learn.\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Function to preprocess the data\n",
    "def extract_data():\n",
    "    # Extracts data from a CSV file\n",
    "    data = pd.read_csv('/Users/charan/Desktop/Lab 3/IMDB dataset.csv')\n",
    "    return data['text'], data['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05bb3839-2295-472e-a268-4ce9628377ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for feature extraction\n",
    "def get_tfidf_vectors(sentence):\n",
    "    \"\"\"\n",
    "    :description: gets the TF-IDF values for the words in each sentence.\n",
    "    :parameters: sentences as strings saved in list.\n",
    "    :return: a list with TF-IDF values for each sentence (will be used as features).\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentence)\n",
    "    print(\"n_samples: %d, n_features: %d\" % vectors.shape)\n",
    "    tf_idf = pd.DataFrame(vectors.todense()).iloc[:len(sentence)]\n",
    "    tf_idf.columns = vectorizer.get_feature_names_out()\n",
    "    tfidf_matrix = tf_idf.T\n",
    "    features = []\n",
    "    for i in tfidf_matrix:\n",
    "       features.append(list(tfidf_matrix[i]))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f05a0f9-f2eb-48b9-a512-c26d53ba8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training the SVM model\n",
    "def svm_classifier(train_features, train_labels, test_features):\n",
    "    clf = SVC(kernel='sigmoid') # Selects and defines the SVM model\n",
    "    clf.fit(train_features, train_labels) # Trains the model\n",
    "    pred = clf.predict(test_features) # Predicts labels of test features\n",
    "    return pred #returns predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950f9113-e642-4d99-b02a-21ad1265dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc4d813-3f45-4cd8-8265-dd26a3aa2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Step 1: Import, Load and preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f5859c7-039b-4ee0-a48f-1e71911e2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c26735b-7aa3-49fc-9288-6d78be572528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cb45adc-03cf-447e-9905-82361dfb75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    data = pd.read_csv('/Users/charan/Desktop/Lab 3/IMDB dataset.csv')\n",
    "    return data['text'], data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c10156-2f96-430a-8a88-f5fb7d7910c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb5a6b2f-59fa-4afe-890c-cf1d72b25b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vectors(sentences):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentences)\n",
    "    return vectors\n",
    "# Custom function to calculate evaluation metrics without using Scikit-Learn's confusion matrix \n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    TP = sum((true_labels == 1) & (predictions == 1))\n",
    "    TN = sum((true_labels == 0) & (predictions == 0))\n",
    "    FP = sum((true_labels == 0) & (predictions == 1))\n",
    "    FN = sum((true_labels == 1) & (predictions == 0))\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    return precision, recall, f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810ce7ff-29a9-4d4a-9d68-e96ba14e396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b33e0fed-a15b-4bad-9930-49a5207ab3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, labels = extract_data()\n",
    "sentence_features = get_tfidf_vectors(sentence)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    sentence_features, labels, test_size=0.2, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65e422d0-e158-4df3-aec4-f08e2381e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c64a271-e64e-43c1-ab45-c5ee5b4e563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_features, train_labels)\n",
    "predictions = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e15d1ba1-dc68-4b67-b942-a281568af0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Calculate Evaluation Metrics with custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a088ee91-21bc-4510-a46a-baaff59c36a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8115079365079365\n",
      "Recall = 0.8131212723658051\n",
      "F1-score = 0.8123138033763654\n",
      "Accuracy = 0.811\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, accuracy = calculate_metrics(test_labels.to_numpy(), predictions)\n",
    "\n",
    "# Output the calculated metrics\n",
    "print(\"Precision =\", precision)\n",
    "print(\"Recall =\", recall)\n",
    "print(\"F1-score =\", f1)\n",
    "print(\"Accuracy =\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e8d5303-0a8f-4b0f-a48b-2bf044035ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a751609-eba1-46d0-b603-4d335e0fe5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT-Based Sentiment Analysis Using Hugging Face\n",
    "#Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4843f72b-ecb3-4f43-83f3-ab0cc5fb0330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5beb4-23ff-47bc-b330-359daecff5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Import Libraries\n",
    "#Step 3: Load and Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c39bcda-9618-4397-b9c4-9e4c32a08159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  It's been about 14 years since Sharon Stone aw...      0\n",
      "1  someone needed to make a car payment... this i...      0\n",
      "2  The Guidelines state that a comment must conta...      0\n",
      "3  This movie is a muddled mish-mash of clichés f...      0\n",
      "4  Before Stan Laurel became the smaller half of ...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('/Users/charan/Desktop/Lab 3/IMDB dataset.csv')\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c571d-8023-4205-88b0-76139cd6828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Initialize the BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "235f7d03-b396-4a69-bcf8-4a8688dd7076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2009,  1005,  ...,  1998,  2585,   102],\n",
       "        [  101,  2619,  2734,  ...,     0,     0,     0],\n",
       "        [  101,  1996, 11594,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2017,  2064,  ...,     0,     0,     0],\n",
       "        [  101, 11865,  5753,  ...,     0,     0,     0],\n",
       "        [  101,  1000,  1996,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text column from the dataset\n",
    "inputs = tokenizer(list(data['text']), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f651d126-a654-4a2e-bd0e-ccf396fb86ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8, \n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9877c4b-7224-4f79-9457-c8b59a1d0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Prepare Custom Dataset Class for PyTorch\n",
    "#Step 6: Split the Dataset into Training and Validation Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "328b14e5-5a37-4adb-b1b9-4cf9a874f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Define a custom dataset class for PyTorch\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Encode the labels and split data\n",
    "labels = list(data['label'])\n",
    "dataset = SentimentDataset(inputs, labels)\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f147daa0-ccc7-4623-8247-b96afe305981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Set Up Training Arguments\n",
    "#Step 8: Initialize the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73069bfe-aa96-41ad-87e9-ff861d94490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=eval_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0f00e54-3f58-4adb-8dff-88357fd97ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/ltpswf3s5v553l99yyc5p8kh0000gn/T/ipykernel_1214/3585948593.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# For predictions and evaluation:\n",
    "# Make predictions on validation set\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c4b2de3-402a-4345-95a9-e0f87c90c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "precision = precision_score(labels, preds, average='weighted')\n",
    "recall = recall_score(labels, preds, average='weighted')\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "accuracy = accuracy_score(labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78ed359e-88f1-4f11-87f6-17fa828c499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5232979502766737\n",
      "Recall: 0.509\n",
      "F1 Score: 0.3547981323884897\n",
      "Accuracy: 0.509\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33226e-2d5b-4b7a-81d0-75d99b73e134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
